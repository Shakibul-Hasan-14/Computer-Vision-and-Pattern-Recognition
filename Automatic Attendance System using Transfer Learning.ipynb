{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob as gb\n",
    "import keras\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\shakib\\Downloads\\Group_Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 2\n",
    "categories = []\n",
    "class_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Category and Barplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_ofsamples = 0\n",
    "\n",
    "for f in os.listdir(data_path):  \n",
    "    files = gb.glob(pathname=str(data_path + '//' + f + '/*'))\n",
    "    categories.append(f)\n",
    "    class_count = len(files)\n",
    "    no_ofsamples += len(files)\n",
    "\n",
    "sns.barplot(x=categories, y=[len(gb.glob(pathname=str(data_path + '//' + f + '/*'))) for f in categories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator (\n",
    "    rotation_range = 40,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    zoom_range = 0.3,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Validation Batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 images belonging to 4 classes.\n",
      "Found 2 images belonging to 4 classes.\n",
      "Training samples: 12\n",
      "Validation samples: 2\n"
     ]
    }
   ],
   "source": [
    "train_batch = train_gen.flow_from_directory(\n",
    "    directory = data_path,\n",
    "    target_size = (IMG_SIZE, IMG_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'sparse',\n",
    "    subset= 'training'\n",
    ")\n",
    "\n",
    "valid_batch =  train_gen.flow_from_directory(\n",
    "    directory = data_path,\n",
    "    target_size = (IMG_SIZE,IMG_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'sparse',\n",
    "    subset= 'validation'\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {train_batch.samples}\")\n",
    "print(f\"Validation samples: {valid_batch.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Batch Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training batch shape: (2, 224, 224, 3), Labels: [0. 2.]\n",
      "First validation batch shape: (2, 224, 224, 3), Labels: [0. 3.]\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = next(train_batch)\n",
    "print(f\"First training batch shape: {batch_x.shape}, Labels: {batch_y}\")\n",
    "\n",
    "batch_x_val, batch_y_val = next(valid_batch)\n",
    "print(f\"First validation batch shape: {batch_x_val.shape}, Labels: {batch_y_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenet_1.00_224 (Function (None, 7, 7, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 7,953,092\n",
      "Trainable params: 7,931,204\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNet(input_shape=img_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "X = base_model(inputs, training=False)\n",
    "X = tf.keras.layers.GlobalAveragePooling2D()(X)\n",
    "\n",
    "X = tf.keras.layers.Dense(2048, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)  \n",
    "X = tf.keras.layers.Dense(1024, activation='relu')(X)  \n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "X = tf.keras.layers.Dense(512, activation='relu')(X)  \n",
    "X = tf.keras.layers.Dropout(0.05)(X)\n",
    "\n",
    "output = tf.keras.layers.Dense(4, activation='softmax')(X)  \n",
    "\n",
    "model = tf.keras.Model(inputs, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile (\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 12s 268ms/step - loss: 1.7925 - accuracy: 0.0000e+00 - val_loss: 1.1542 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 1.3917 - accuracy: 0.2500 - val_loss: 1.1177 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.2212 - accuracy: 0.5833 - val_loss: 0.8944 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 0.9979 - accuracy: 0.8333 - val_loss: 1.0587 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 0.7722 - accuracy: 0.8333 - val_loss: 0.7374 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.8398 - accuracy: 0.7500 - val_loss: 0.6192 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.7401 - accuracy: 0.6667 - val_loss: 0.5513 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.5931 - accuracy: 0.7500 - val_loss: 0.8089 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.5202 - accuracy: 1.0000 - val_loss: 0.3936 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.4910 - accuracy: 0.9167 - val_loss: 0.3727 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(\n",
    "    train_batch,  \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=6, \n",
    "    validation_data=valid_batch,  \n",
    "    validation_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webcam Capture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a picture in 5 seconds...\n",
      "Predicted Student: SUN\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Taking a picture in 5 seconds...\")\n",
    "\n",
    "time.sleep(5)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Failed to capture image from webcam. Exiting...\")\n",
    "else:\n",
    "    # Preprocess the frame (resize and normalize)\n",
    "    img = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    \n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions, axis=-1)\n",
    "    student_name = categories[predicted_class[0]]\n",
    "    \n",
    "    cv2.putText(frame, f\"Student: {student_name}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Captured Image - Prediction', frame)\n",
    "    \n",
    "    print(f\"Predicted Student: {student_name}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
